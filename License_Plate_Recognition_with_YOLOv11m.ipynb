{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1203932,
          "sourceType": "datasetVersion",
          "datasetId": 686454
        },
        {
          "sourceId": 4796468,
          "sourceType": "datasetVersion",
          "datasetId": 2776891
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "License Plate Recognition with YOLOv11m",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semihberat/YOLOv11-NumberPlateRecognition/blob/main/License_Plate_Recognition_with_YOLOv11m.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "andrewmvd_car_plate_detection_path = kagglehub.dataset_download('andrewmvd/car-plate-detection')\n",
        "smaildurcan_turkish_license_plate_dataset_path = kagglehub.dataset_download('smaildurcan/turkish-license-plate-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "URfILVk3eb15"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T20:09:35.532154Z",
          "iopub.execute_input": "2025-05-14T20:09:35.532423Z",
          "iopub.status.idle": "2025-05-14T20:09:38.979948Z",
          "shell.execute_reply.started": "2025-05-14T20:09:35.532405Z",
          "shell.execute_reply": "2025-05-14T20:09:38.978941Z"
        },
        "id": "bQHBJAXqeb17"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEPENDENCIES**"
      ],
      "metadata": {
        "id": "-8XUbb2Deb18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from glob import glob\n",
        "from xml.dom import minidom\n",
        "import random\n",
        "import easyocr\n",
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import pytesseract\n",
        "from PIL import Image"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:25:33.93301Z",
          "iopub.execute_input": "2025-05-14T19:25:33.933302Z",
          "iopub.status.idle": "2025-05-14T19:25:42.14162Z",
          "shell.execute_reply.started": "2025-05-14T19:25:33.933277Z",
          "shell.execute_reply": "2025-05-14T19:25:42.141085Z"
        },
        "id": "ZOJMxf3Web1-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `convert_xml2yolo` Function\n",
        "\n",
        "![image.png](attachment:36a8cebd-5bfb-401c-b5dc-cc388948e059.png)\n",
        "\n",
        "## Overview\n",
        "The `convert_xml2yolo` function is designed to convert object detection annotations from XML format (commonly used in datasets like Pascal VOC) to YOLO format. This conversion is essential for training object detection models using the YOLO framework, which requires annotations in a specific format.\n",
        "\n",
        "## Function Signature\n",
        "```python\n",
        "def convert_xml2yolo(lut, input_path, output_path):\n",
        "```\n",
        "\n",
        "### Parameters\n",
        "- **`lut`**: A dictionary that maps class names (strings) to numeric labels (integers). This is used to assign YOLO-compatible class IDs to the objects in the annotations.\n",
        "- **`input_path`**: The directory containing the XML annotation files.\n",
        "- **`output_path`**: The directory where the converted YOLO annotation files will be saved.\n",
        "\n",
        "## Workflow\n",
        "1. **Check Output Directory**:\n",
        "   - If the specified `output_path` does not exist, it is created using `os.mkdir`.\n",
        "\n",
        "2. **Iterate Over XML Files**:\n",
        "   - The function uses the `glob` module to find all XML files in the `input_path` directory.\n",
        "\n",
        "3. **Parse XML File**:\n",
        "   - Each XML file is parsed using the `minidom` module to extract annotation details.\n",
        "\n",
        "4. **Extract Image Dimensions**:\n",
        "   - The width and height of the image are retrieved from the `<size>` tag in the XML file.\n",
        "\n",
        "5. **Process Each Object**:\n",
        "   - For each `<object>` tag in the XML file:\n",
        "     - The class name is mapped to a numeric label using the `lut` dictionary.\n",
        "     - The bounding box coordinates (`xmin`, `ymin`, `xmax`, `ymax`) are extracted and converted to YOLO format using the `convert_coordinates` helper function.\n",
        "\n",
        "6. **Write YOLO Annotation File**:\n",
        "   - The converted annotations are written to a `.txt` file in the `output_path` directory.\n",
        "\n",
        "7. **Completion Message**:\n",
        "   - A message is printed to indicate that the conversion process is finished.\n",
        "\n",
        "## Helper Function: `convert_coordinates`\n",
        "The `convert_coordinates` function is used to transform bounding box coordinates from absolute pixel values to normalized YOLO format.\n",
        "\n",
        "### Formula\n",
        "Given:\n",
        "- Image dimensions: `(width, height)`\n",
        "- Bounding box: `(xmin, xmax, ymin, ymax)`\n",
        "\n",
        "The YOLO format is calculated as:\n",
        "- `x_center = (xmin + xmax) / 2 / width`\n",
        "- `y_center = (ymin + ymax) / 2 / height`\n",
        "- `box_width = (xmax - xmin) / width`\n",
        "- `box_height = (ymax - ymin) / height`\n",
        "\n",
        "### Code\n",
        "```python\n",
        "def convert_coordinates(size, box):\n",
        "    dw = 1.0 / size[0]\n",
        "    dh = 1.0 / size[1]\n",
        "    x = (box[0] + box[1]) / 2.0\n",
        "    y = (box[2] + box[3]) / 2.0\n",
        "    w = box[1] - box[0]\n",
        "    h = box[3] - box[2]\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return (x, y, w, h)\n",
        "```\n",
        "\n",
        "## Example Usage\n",
        "```python\n",
        "lut = {\n",
        "    \"car\": 0,\n",
        "    \"person\": 1,\n",
        "    \"bicycle\": 2\n",
        "}\n",
        "\n",
        "convert_xml2yolo(\n",
        "    lut,\n",
        "    input_path=\"/path/to/xml/annotations\",\n",
        "    output_path=\"/path/to/yolo/annotations\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Notes\n",
        "- If a class name in the XML file is not found in the `lut` dictionary, it is assigned a default label of `0`.\n",
        "- The function assumes that the XML files follow the Pascal VOC format.\n",
        "\n",
        "## Output Format\n",
        "Each line in the YOLO annotation file corresponds to an object and follows the format:\n",
        "```\n",
        "<class_id> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "All values are normalized to the range `[0, 1]`.\n",
        "\n",
        "## Conclusion\n",
        "The `convert_xml2yolo` function simplifies the process of preparing annotation data for YOLO-based object detection models. By automating the conversion from XML to YOLO format, it ensures compatibility and reduces manual effort."
      ],
      "metadata": {
        "id": "f7IL6Wdneb1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "lut={}\n",
        "lut[\"accessory\"] =0\n",
        "lut[\"top\"]       =1\n",
        "lut[\"bottom\"]    =2\n",
        "lut[\"bag\"]       =3\n",
        "lut[\"shoes\"]     =4\n",
        "\n",
        "\n",
        "\n",
        "def convert_coordinates(size, box):\n",
        "    dw = 1.0/size[0]\n",
        "    dh = 1.0/size[1]\n",
        "    x = (box[0]+box[1])/2.0\n",
        "    y = (box[2]+box[3])/2.0\n",
        "    w = box[1]-box[0]\n",
        "    h = box[3]-box[2]\n",
        "    x = x*dw\n",
        "    w = w*dw\n",
        "    y = y*dh\n",
        "    h = h*dh\n",
        "    return (x,y,w,h)\n",
        "\n",
        "\n",
        "def convert_xml2yolo( lut,input_path, output_path ):\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "         os.mkdir(output_path)\n",
        "\n",
        "    for fname in glob(f\"{input_path}/*.xml\"):\n",
        "\n",
        "        xmldoc = minidom.parse(fname)\n",
        "        annot_fname = fname.split(\"/\")[-1][:-4]\n",
        "        fname_out = f\"{output_path}/{annot_fname}.txt\"\n",
        "\n",
        "        with open(fname_out, \"w\") as f:\n",
        "\n",
        "            itemlist = xmldoc.getElementsByTagName('object')\n",
        "            size = xmldoc.getElementsByTagName('size')[0]\n",
        "            width = int((size.getElementsByTagName('width')[0]).firstChild.data)\n",
        "            height = int((size.getElementsByTagName('height')[0]).firstChild.data)\n",
        "\n",
        "            for item in itemlist:\n",
        "                # get class label\n",
        "                classid =  (item.getElementsByTagName('name')[0]).firstChild.data\n",
        "                if classid in lut:\n",
        "                    label_str = str(lut[classid])\n",
        "                else:\n",
        "                    label_str = \"0\"\n",
        "                    #print (\"warning: label '%s' not in look-up table\" % classid)\n",
        "\n",
        "                # get bbox coordinates\n",
        "                xmin = ((item.getElementsByTagName('bndbox')[0]).getElementsByTagName('xmin')[0]).firstChild.data\n",
        "                ymin = ((item.getElementsByTagName('bndbox')[0]).getElementsByTagName('ymin')[0]).firstChild.data\n",
        "                xmax = ((item.getElementsByTagName('bndbox')[0]).getElementsByTagName('xmax')[0]).firstChild.data\n",
        "                ymax = ((item.getElementsByTagName('bndbox')[0]).getElementsByTagName('ymax')[0]).firstChild.data\n",
        "                b = (float(xmin), float(xmax), float(ymin), float(ymax))\n",
        "                bb = convert_coordinates((width,height), b)\n",
        "                #print(bb)\n",
        "\n",
        "                f.write(label_str + \" \" + \" \".join([(\"%.6f\" % a) for a in bb]) + '\\n')\n",
        "\n",
        "        #print (\"wrote %s\" % fname_out)\n",
        "    print(\"Converting is finished!\")\n",
        "\n",
        "convert_xml2yolo( lut, input_path = \"/kaggle/input/car-plate-detection/annotations\", output_path = \"/kaggle/working/annotations\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T20:11:02.487729Z",
          "iopub.execute_input": "2025-05-14T20:11:02.488325Z",
          "iopub.status.idle": "2025-05-14T20:11:03.367297Z",
          "shell.execute_reply.started": "2025-05-14T20:11:02.488294Z",
          "shell.execute_reply": "2025-05-14T20:11:03.366605Z"
        },
        "id": "kG5hWJmVeb2B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VAL & TRAIN SPLIT PATHS**"
      ],
      "metadata": {
        "id": "6LEB8yvyeb2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directories_if_not_exist(directories: list):\n",
        "    # Assert: Is the parameter a list?\n",
        "    assert isinstance(directories, list), \"The parameter must be a list (array).\"\n",
        "\n",
        "    # Assert: Is every element in the list a string?\n",
        "    for dir_path in directories:\n",
        "        assert isinstance(dir_path, str), f\"The element '{dir_path}' in the list is not a string.\"\n",
        "\n",
        "    # Create directories\n",
        "    for dir_path in directories:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.mkdir(dir_path)\n",
        "            print(f\"Created: {dir_path}\")\n",
        "        else:\n",
        "            print(f\"Already exists: {dir_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T20:11:04.989787Z",
          "iopub.execute_input": "2025-05-14T20:11:04.990365Z",
          "iopub.status.idle": "2025-05-14T20:11:04.994759Z",
          "shell.execute_reply.started": "2025-05-14T20:11:04.990342Z",
          "shell.execute_reply": "2025-05-14T20:11:04.99419Z"
        },
        "id": "1c9_9mwFeb2D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "directories = [\n",
        "    \"/kaggle/working/car_plate_dataset\",\n",
        "    \"/kaggle/working/car_plate_dataset/images\",\n",
        "    \"/kaggle/working/car_plate_dataset/labels\",\n",
        "    \"/kaggle/working/car_plate_dataset/images/train\",\n",
        "    \"/kaggle/working/car_plate_dataset/images/val\",\n",
        "    \"/kaggle/working/car_plate_dataset/labels/train\",\n",
        "    \"/kaggle/working/car_plate_dataset/labels/val\"\n",
        "]\n",
        "\n",
        "create_directories_if_not_exist(directories)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:25:44.852422Z",
          "iopub.execute_input": "2025-05-14T19:25:44.852683Z",
          "iopub.status.idle": "2025-05-14T19:25:44.86313Z",
          "shell.execute_reply.started": "2025-05-14T19:25:44.852659Z",
          "shell.execute_reply": "2025-05-14T19:25:44.862538Z"
        },
        "id": "jh2UhKrfeb2E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `split_images_and_labels` Function\n",
        "\n",
        "## Overview\n",
        "The `split_images_and_labels` function splits a dataset of images and labels into training and validation sets, useful for machine learning tasks like object detection.\n",
        "\n",
        "## Parameters\n",
        "- `val_size` (float): Proportion of the dataset for validation (default: 0.1).\n",
        "- `input_dir` (str): Directory containing `images/` and `annotations/` subfolders.\n",
        "- `output_dir` (str): Directory where the split dataset will be saved.\n",
        "\n",
        "## Workflow\n",
        "1. **Splitting:** Images are divided into `train` and `val` sets based on `val_size`.\n",
        "2. **Copying:** Files are copied to `output_dir` under `images/train`, `images/val`, `labels/train`, and `labels/val`.\n",
        "3. **Completion:** Prints a message when done.\n",
        "\n",
        "## Example Usage\n",
        "```python\n",
        "split_images_and_labels(\n",
        "    val_size=0.2,\n",
        "    input_dir=\"/path/to/input\",\n",
        "    output_dir=\"/path/to/output\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Notes\n",
        "- Assumes matching filenames for images and labels (e.g., `image1.jpg` and `image1.txt`).\n",
        "- Creates `output_dir` if it doesn’t exist.\n",
        "\n",
        "## Output Example\n",
        "For `val_size=0.5`:\n",
        "```plaintext\n",
        "output_dir/\n",
        "    images/\n",
        "        train/\n",
        "            car1.jpg\n",
        "        val/\n",
        "            car2.jpg\n",
        "    labels/\n",
        "        train/\n",
        "            car1.txt\n",
        "        val/\n",
        "            car2.txt\n",
        "```\n"
      ],
      "metadata": {
        "id": "yyXfCpFJeb2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_images_and_labels(val_size = 0.1, input_dir = \"/kaggle/input/car-plate-detection\",\n",
        "                           output_dir = \"/kaggle/working/car_plate_dataset\"):\n",
        "\n",
        "        img_path = os.listdir(f\"{input_dir}/images\")\n",
        "        label_path = os.listdir(f\"{input_dir}/annotations\")\n",
        "        imgs_length = len(img_path)\n",
        "        for i,img in enumerate(img_path):\n",
        "            spname = \"train\" if i < int(imgs_length*float(1 - val_size)) else \"val\"\n",
        "\n",
        "            #print(spname)\n",
        "\n",
        "            os.system(f\"cp {input_dir}/images/{img} {output_dir}/images/{spname}/{img}\")\n",
        "            #print(f\"the image {input_dir}/images/{img} image copied to {output_dir}/images/{spname}/{img}\")\n",
        "\n",
        "            os.system(f\"cp /kaggle/working/annotations/{img.split('.')[0]}.txt {output_dir}/labels/{spname}/{img.split('.')[0]}.txt\")\n",
        "            #print(f\"the label /kaggle/working/annotations/{img.split('.')[0]}.txt image copied to {output_dir}/labels/{spname}/{img.split('.')[0]}.txt\")\n",
        "\n",
        "        print(\"Splitting is finished!\")\n",
        "\n",
        "split_images_and_labels()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:25:44.863808Z",
          "iopub.execute_input": "2025-05-14T19:25:44.864013Z",
          "iopub.status.idle": "2025-05-14T19:25:53.599709Z",
          "shell.execute_reply.started": "2025-05-14T19:25:44.863998Z",
          "shell.execute_reply": "2025-05-14T19:25:53.598952Z"
        },
        "id": "qWBo3W3Heb2G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **YAML PARAMETERS**"
      ],
      "metadata": {
        "id": "pSwDGsG7eb2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data to be written to the YAML file\n",
        "data = {\n",
        "    'train': '/kaggle/working/car_plate_dataset/images/train',\n",
        "    'val': '/kaggle/working/car_plate_dataset/images/val',\n",
        "    'names': {\n",
        "        0: \"number_plate\"\n",
        "    } # List formatında olmalı!\n",
        "}\n",
        "\n",
        "# Writing the data to a YAML file\n",
        "with open('data.yaml', 'w') as file:\n",
        "    yaml.dump(data, file, default_flow_style=False)\n",
        "\n",
        "print(\"Data has been written to 'data.yaml'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:25:53.600669Z",
          "iopub.execute_input": "2025-05-14T19:25:53.60099Z",
          "iopub.status.idle": "2025-05-14T19:25:53.607417Z",
          "shell.execute_reply.started": "2025-05-14T19:25:53.600966Z",
          "shell.execute_reply": "2025-05-14T19:25:53.606583Z"
        },
        "id": "8MbtCIt7eb2H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "\n",
        "## Overview\n",
        "This script visualizes a grid of images with bounding boxes drawn from YOLO annotations. It is useful for verifying the correctness of annotations and inspecting the dataset.\n",
        "\n",
        "## Workflow\n",
        "1. **File Collection:**\n",
        "   - Collects image files from the specified directory (`image_dir`) with supported extensions (`*.jpg`, `*.jpeg`, `*.png`).\n",
        "   - Selects the first 9 images for visualization.\n",
        "\n",
        "2. **Grid Creation:**\n",
        "   - Creates a 3x3 grid using Matplotlib to display the images.\n",
        "\n",
        "3. **Bounding Box Drawing:**\n",
        "   - For each image, the corresponding YOLO annotation file is read.\n",
        "   - Bounding boxes are converted from YOLO format to pixel coordinates and drawn on the image.\n",
        "\n",
        "4. **Display:**\n",
        "   - Displays the grid of images with bounding boxes using Matplotlib.\n",
        "\n",
        "## Key Variables\n",
        "- `image_dir`: Path to the directory containing the images.\n",
        "- `label_dir`: Path to the directory containing the YOLO annotation files.\n",
        "- `image_extensions`: List of supported image file extensions.\n",
        "\n",
        "## Example Usage\n",
        "```python\n",
        "# File paths\n",
        "image_dir = \"/path/to/images\"\n",
        "label_dir = \"/path/to/labels\"\n",
        "\n",
        "# Supported extensions\n",
        "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "\n",
        "# Visualization script\n",
        "# (Refer to the script in `main.py` for full implementation)\n",
        "```\n",
        "\n",
        "## Notes\n",
        "- The script assumes that the filenames of the images and their corresponding labels match (e.g., `image1.jpg` and `image1.txt`).\n",
        "- Bounding boxes are drawn in red for better visibility.\n",
        "- Ensure that the `image_dir` and `label_dir` paths are correctly set before running the script.\n",
        "\n",
        "## Output Example\n",
        "The script generates a 3x3 grid of images with bounding boxes drawn, as shown below:\n",
        "\n",
        "```\n",
        "+---------+---------+---------+\n",
        "| Image 1 | Image 2 | Image 3 |\n",
        "+---------+---------+---------+\n",
        "| Image 4 | Image 5 | Image 6 |\n",
        "+---------+---------+---------+\n",
        "| Image 7 | Image 8 | Image 9 |\n",
        "+---------+---------+---------+\n",
        "```\n",
        "\n",
        "Bounding boxes are overlaid on the images in red.\n"
      ],
      "metadata": {
        "id": "Hi_xfyajeb2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "image_dir = \"/kaggle/working/car_plate_dataset/images/train\"\n",
        "label_dir = \"/kaggle/working/car_plate_dataset/labels/train\"\n",
        "\n",
        "# Supported extensions\n",
        "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
        "\n",
        "# Collect all image files\n",
        "image_paths = []\n",
        "for ext in image_extensions:\n",
        "    image_paths.extend(glob(os.path.join(image_dir, ext)))\n",
        "\n",
        "# Use the first 9 images\n",
        "image_paths = sorted(image_paths)[:9]\n",
        "\n",
        "# Draw a 3x3 grid\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, image_path in enumerate(image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    # Find the corresponding label file (change extension to .txt)\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    label_path = os.path.join(label_dir, base_name + \".txt\")\n",
        "\n",
        "    # Draw with Matplotlib\n",
        "    ax = axes[idx]\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Draw boxes if labels exist\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "\n",
        "            # Convert from YOLO format to pixels\n",
        "            x1 = int((x_center - box_width / 2) * w)\n",
        "            y1 = int((y_center - box_height / 2) * h)\n",
        "            x2 = int((x_center + box_width / 2) * w)\n",
        "            y2 = int((y_center + box_height / 2) * h)\n",
        "\n",
        "            # Draw rectangle\n",
        "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                     linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:25:53.608358Z",
          "iopub.execute_input": "2025-05-14T19:25:53.608584Z",
          "iopub.status.idle": "2025-05-14T19:25:54.926304Z",
          "shell.execute_reply.started": "2025-05-14T19:25:53.608557Z",
          "shell.execute_reply": "2025-05-14T19:25:54.925244Z"
        },
        "id": "ZRXVxmCeeb2I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO Training\n",
        "\n",
        "## Overview of YOLO\n",
        "YOLO (You Only Look Once) is a state-of-the-art object detection algorithm that performs detection in a single pass through the network. Unlike traditional methods that use region proposals and multiple stages, YOLO treats object detection as a regression problem, predicting bounding boxes and class probabilities directly from the input image.\n",
        "\n",
        "### Key Features of YOLO\n",
        "1. **Speed:** YOLO is extremely fast because it processes the entire image in a single forward pass.\n",
        "2. **Accuracy:** It achieves high accuracy by learning global image context and spatial relationships.\n",
        "3. **Unified Architecture:** YOLO uses a single convolutional neural network (CNN) to predict bounding boxes and class probabilities simultaneously.\n",
        "\n",
        "## Explanation of IoU and mAP Metrics\n",
        "\n",
        "### Intersection over Union (IoU)\n",
        "IoU measures the overlap between two bounding boxes, typically the predicted and ground truth boxes. It is defined as:\n",
        "$$ IoU = \\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union} $$\n",
        "\n",
        "- **Area of Overlap:** The area where the predicted and ground truth boxes intersect.\n",
        "- **Area of Union:** The total area covered by both boxes.\n",
        "\n",
        "IoU ranges from 0 to 1, where 1 indicates perfect overlap. It is used to determine whether a predicted bounding box is a true positive or a false positive.\n",
        "\n",
        "### Mean Average Precision (mAP)\n",
        "mAP evaluates the performance of object detection models by calculating the average precision (AP) for each class and then taking the mean.\n",
        "\n",
        "#### Average Precision (AP)\n",
        "AP is the area under the Precision-Recall curve for a specific class. It is calculated as:\n",
        "$$ AP = \\int_0^1 P(R) dR $$\n",
        "\n",
        "- **P(R):** Precision as a function of Recall.\n",
        "\n",
        "#### Mean Average Precision\n",
        "The mAP is then calculated as:\n",
        "$$ mAP = \\frac{1}{N} \\sum_{i=1}^{N} AP_i $$\n",
        "\n",
        "- **N:** Total number of classes.\n",
        "- **AP_i:** Average Precision for class $i$.\n",
        "\n",
        "## Explanation of the Training Code\n",
        "\n",
        "### GPU Configuration\n",
        "```python\n",
        "gpu_count = torch.cuda.device_count()\n",
        "device = list(range(gpu_count)) if gpu_count > 1 else 0\n",
        "```\n",
        "- **`torch.cuda.device_count()`:** Checks the number of available GPUs.\n",
        "- **`device`:** Sets the device to use multiple GPUs if available, otherwise defaults to a single GPU.\n",
        "\n",
        "### Model Initialization\n",
        "```python\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "```\n",
        "- **`YOLO`:** Initializes the YOLO model with the specified weights file (`yolo11n.pt`).\n",
        "\n",
        "### Training Configuration\n",
        "```python\n",
        "results = model.train(\n",
        "    data=\"/kaggle/working/data.yaml\",   # Dataset configuration\n",
        "    epochs=200,                         # 200 epochs\n",
        "    imgsz=640,                          # Suitable for smaller images\n",
        "    batch=32,                           # Adjustable based on GPU RAM\n",
        "    workers=2,                          # Ideal starting value for Tesla T4\n",
        "    device=device,                      # GPU setting\n",
        "    augment=True,                       # Default YOLO augmentations are automatically enabled\n",
        "    patience=20,                        # Stops early if no improvement for 20 epochs\n",
        "    val=True,                           # Validation is performed at the end of each epoch\n",
        ")\n",
        "```\n",
        "#### Key Parameters\n",
        "- **`data`:** Path to the dataset configuration file (`data.yaml`).\n",
        "- **`epochs`:** Number of training epochs (200 in this case).\n",
        "- **`imgsz`:** Image size for training (640x640 pixels).\n",
        "- **`batch`:** Batch size, adjustable based on GPU memory.\n",
        "- **`workers`:** Number of data loader workers (2 is ideal for Tesla T4 GPUs).\n",
        "- **`device`:** Specifies the GPU(s) to use.\n",
        "- **`augment`:** Enables default YOLO augmentations for data augmentation.\n",
        "- **`patience`:** Early stopping if no improvement for 20 epochs.\n",
        "- **`val`:** Enables validation at the end of each epoch.\n",
        "\n",
        "## Notes\n",
        "- Ensure that the dataset is correctly formatted and the `data.yaml` file is properly configured.\n",
        "- Adjust the `batch` size and `workers` based on the available GPU resources.\n",
        "- Use a pre-trained weights file (e.g., `yolo11n.pt`) to speed up training and improve accuracy.\n",
        "\n",
        "## Output\n",
        "The training process outputs:\n",
        "1. **Model Weights:** Saved at regular intervals and at the end of training.\n",
        "2. **Metrics:** Training and validation loss, mAP, and other performance metrics.\n",
        "3. **Logs:** Detailed logs for each epoch, including loss and mAP values.\n",
        "\n",
        "This documentation provides a comprehensive explanation of the YOLO training process, including the metrics and code used. Let me know if you need further clarifications or additional details!\n"
      ],
      "metadata": {
        "id": "F0kWvGn6eb2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of GPUs\n",
        "gpu_count = torch.cuda.device_count()\n",
        "device = list(range(gpu_count)) if gpu_count > 1 else 0\n",
        "\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data=\"/kaggle/working/data.yaml\",   # Dataset configuration\n",
        "    epochs=200,                         # 200 epochs\n",
        "    imgsz=640,                          # Suitable for smaller images\n",
        "    batch=32,                            # Adjustable based on GPU RAM\n",
        "    workers=2,                          # Ideal starting value for Tesla T4\n",
        "    device=device,                      # GPU setting\n",
        "\n",
        "    # ✅ Default Regularization & Optimization (YOLO uses its own defaults)\n",
        "    # If you don't define these parameters, defaults will be used.\n",
        "\n",
        "    # ✅ Default Augmentations\n",
        "    augment=True,                       # Default YOLO augmentations are automatically enabled\n",
        "\n",
        "    # ✅ Early Stopping and Validation\n",
        "    patience=20,                        # Stops early if no improvement for 10 epochs\n",
        "    val=True,                           # Validation is performed at the end of each epoch\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:43:32.339725Z",
          "iopub.execute_input": "2025-05-14T19:43:32.340709Z",
          "iopub.status.idle": "2025-05-14T19:50:03.714021Z",
          "shell.execute_reply.started": "2025-05-14T19:43:32.34066Z",
          "shell.execute_reply": "2025-05-14T19:50:03.713143Z"
        },
        "id": "aRwePmZweb2J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# License Plate Detection and OCR Documentation\n",
        "\n",
        "## Overview\n",
        "This script performs license plate detection and Optical Character Recognition (OCR) on a set of images. It uses the YOLO object detection model to locate license plates and Tesseract OCR to extract text from the detected plates. The script is designed for Turkish license plates and includes preprocessing steps to improve OCR accuracy.\n",
        "\n",
        "## Workflow\n",
        "1. **Image Loading:**\n",
        "   - Randomly selects a subset of images from the dataset.\n",
        "   - Reads and converts images to RGB format for processing.\n",
        "\n",
        "2. **License Plate Detection:**\n",
        "   - Uses a YOLO model to detect license plates in the images.\n",
        "   - Extracts bounding box coordinates for each detected plate.\n",
        "\n",
        "3. **Preprocessing:**\n",
        "   - Converts the license plate region to grayscale.\n",
        "   - Crops the left side of the plate (removing the blue \"TR\" section).\n",
        "   - Applies Gaussian blur and thresholding to enhance text clarity.\n",
        "\n",
        "4. **OCR:**\n",
        "   - Uses Tesseract OCR to extract text from the preprocessed license plate region.\n",
        "   - Formats the extracted text to match Turkish license plate standards.\n",
        "\n",
        "5. **Visualization:**\n",
        "   - Draws bounding boxes and recognized text on the images.\n",
        "   - Displays the processed images with Matplotlib.\n",
        "\n",
        "6. **Output:**\n",
        "   - Prints the recognized license plate text for each image.\n",
        "\n",
        "## Key Functions and Code Snippets\n",
        "\n",
        "### YOLO Model Initialization\n",
        "```python\n",
        "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
        "```\n",
        "- Loads the YOLO model with pre-trained weights.\n",
        "- Used for detecting license plates in images.\n",
        "\n",
        "### OCR Preprocessing\n",
        "```python\n",
        "plate_img = img[y1:y2, x1:x2]\n",
        "gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n",
        "cut_x = int(w * 0.10)  # Remove 10% from the left\n",
        "gray = gray[:, cut_x:]\n",
        "blurred = cv2.GaussianBlur(gray, (5, 5), 1)\n",
        "_, thresh = cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "```\n",
        "- Extracts the license plate region from the image.\n",
        "- Converts the region to grayscale and removes the left side.\n",
        "- Applies Gaussian blur and thresholding to prepare the region for OCR.\n",
        "\n",
        "### OCR with Tesseract\n",
        "```python\n",
        "text = pytesseract.image_to_string(thresh, config=custom_config)\n",
        "formatted_text = format_turkish_plate(text)\n",
        "```\n",
        "- Extracts text from the preprocessed license plate region.\n",
        "- Formats the text to match Turkish license plate standards.\n",
        "\n",
        "### Visualization\n",
        "```python\n",
        "cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "cv2.putText(img_rgb, formatted_text, (x1, y1 - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "```\n",
        "- Draws a bounding box around the detected license plate.\n",
        "- Adds the recognized text above the bounding box.\n",
        "\n",
        "### Displaying Results\n",
        "```python\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(img_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "```\n",
        "- Displays the processed image with bounding boxes and recognized text.\n",
        "\n",
        "## Example Usage\n",
        "```python\n",
        "# Load images\n",
        "image_paths = glob(\"/path/to/images/*.jpg\")\n",
        "random_images = random.sample(image_paths, 10)\n",
        "\n",
        "# Process each image\n",
        "for img_path in random_images:\n",
        "    img = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = model(img_path, conf=0.4, iou=0.4)\n",
        "    # Further processing as described above\n",
        "```\n",
        "\n",
        "## Notes\n",
        "- Ensure that the YOLO model weights and Tesseract OCR are correctly installed and configured.\n",
        "- The script is optimized for Turkish license plates but can be adapted for other formats with minor modifications.\n",
        "- Adjust the confidence (`conf`) and IoU thresholds for YOLO as needed.\n",
        "\n",
        "## Output Example\n",
        "For an image with a detected license plate, the script outputs:\n",
        "- The processed image with bounding boxes and recognized text.\n",
        "- The recognized license plate text printed to the console:\n",
        "  ```\n",
        "  Plate 1: 34ABC123\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "eMyRjtWKeb2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import random\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Tesseract ayarları\n",
        "custom_config = r'--oem 3 --psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "\n",
        "# YOLO modelini yükle\n",
        "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
        "\n",
        "# Görüntüleri al\n",
        "image_paths = glob(\"/kaggle/input/turkish-license-plate-dataset/images/*.png\") + \\\n",
        "              glob(\"/kaggle/input/turkish-license-plate-dataset/images/*.jpg\")\n",
        "\n",
        "num_images = min(10, len(image_paths))\n",
        "random_images = random.sample(image_paths, num_images)\n",
        "\n",
        "# Boşluksuz Türk plaka formatlayıcı\n",
        "def format_turkish_plate(text):\n",
        "    return text.strip().upper().replace(\" \", \"\").replace(\"-\", \"\")\n",
        "\n",
        "# Görüntüler üzerinde işlem yap\n",
        "for img_path in random_images:\n",
        "    img = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    results = model(img_path, conf=0.4, iou=0.4)\n",
        "    detected_text_list = []\n",
        "\n",
        "    for result in results:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
        "\n",
        "            # Plaka bölgesini al ve ön işleme uygula\n",
        "            plate_img = img[y1:y2, x1:x2]\n",
        "            gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Sol taraf (mavi TR kısmı) kesiliyor\n",
        "            h, w = gray.shape\n",
        "            cut_x = int(w * 0.10)  # %20'sini kes\n",
        "            gray = gray[:, cut_x:]\n",
        "\n",
        "            # Orta düzey Gaussian Blur\n",
        "            blurred = cv2.GaussianBlur(gray, (5, 5), 1)\n",
        "\n",
        "            # Threshold ile netleştirme\n",
        "            _, thresh = cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "            # OCR işlemi\n",
        "            text = pytesseract.image_to_string(thresh, config=custom_config)\n",
        "            formatted_text = format_turkish_plate(text)\n",
        "\n",
        "            # Eğer plaka uzunluğu yeterliyse kutu çiz ve yazıyı ekle\n",
        "            if len(formatted_text) >= 6:\n",
        "                cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
        "                cv2.putText(img_rgb, formatted_text, (x1, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "                detected_text_list.append(formatted_text)\n",
        "\n",
        "    # Görüntüyü göster\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # Tanınan plakaları yazdır\n",
        "    for i, text in enumerate(detected_text_list):\n",
        "        print(f\"Plate {i + 1}: {text}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T19:55:38.216234Z",
          "iopub.execute_input": "2025-05-14T19:55:38.216512Z",
          "iopub.status.idle": "2025-05-14T19:55:48.885625Z",
          "shell.execute_reply.started": "2025-05-14T19:55:38.216494Z",
          "shell.execute_reply": "2025-05-14T19:55:48.884829Z"
        },
        "id": "31F1Ckz1eb2K"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}